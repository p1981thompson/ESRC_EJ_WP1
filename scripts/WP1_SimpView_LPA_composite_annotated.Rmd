---
title: 'WP1: Simple View - Latent Profile Analysis'
output: 
  html_document:
    toc: true
    toc_float: true
date: '12/08/2021'
    
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../output/analysis/") })
editor_options: 
  markdown: 
    wrap: 160
    canonical: true
---

This script is used for running the latent profile models. This script (rather than the factor mixture model alternative WP1_SimpView_FMM.Rmd) was selected on
the basis of the CFA results, which found that the two-factor structure was not a good fit. This analysis is conducted using the "exploratory" or "calibration"
half of the dataset (subsample A).

The pre-registration for this analysis can be found [here](https://osf.io/4zahf).

# Set-up

## Libraries

*Package versions were up-to-date as of 12/08/2021. Version information can be found at the [end of the output file](#version).*

*Models were run using Mplus Version 8.5.*

```{r libraries, message = FALSE, warnings = FALSE}
# Specify packages
pkgs <- c("tidyverse", "MplusAutomation", "texreg", "DiagrammeR", "kableExtra", "ggalluvial", "scales", "rlist", "here")

# Load all packages
invisible(lapply(pkgs, library, character.only = TRUE))
```

## Directories

```{r create-dir}
# Create subdirectories for storing mplus scripts, data files, and output, if do not already exist
if(dir.exists("./mplus_models/")==FALSE){dir.create("./mplus_models/")}
if(dir.exists("./mplus_models/simpview/")==FALSE){dir.create("./mplus_models/simpview/")}
if(dir.exists("./mplus_models/simpview/lpa_comp")==FALSE){dir.create("./mplus_models/simpview/lpa_comp")} 
if(dir.exists("../output/")==FALSE){dir.create("../output/")}
if(dir.exists("../output/figures")==FALSE){dir.create("../output/figures")}
if(dir.exists("../output/tables")==FALSE){dir.create("../output/tables")}
```

## Functions

Functions for processing the output are stored in a separate script.

```{r output-functions}
source("WP1_OutputFunctions.R")
```

## Data

This analysis is developed on half of the dataset, for subsequent cross-validation with the remaining half (WP1_SimpView_CrossValidation.Rmd).

```{r load-data}
# sv_data <- read.csv("../data/simulated/WP1_SimpView_sim_raw_n1000_k3.csv")  # for script testing

sv_data <- read.csv("../data/processed/WP1_data_subA.csv")  %>%       # ALSPAC data
  
  # Select and format relevant variables
  select(yp_id, cidB3153, age_m_f8, age_m_f9, 
                nara_acc_raw_f9, read_word_raw_f9, read_nonw_raw_f9,  # reading accuracy variables
                nara_comp_raw_f9, wold_comp_raw_f8) %>%               # comprehension variables
  mutate(combAcc = read_word_raw_f9 + read_nonw_raw_f9) %>%           # create combined item accuracy score
  select(-read_word_raw_f9, -read_nonw_raw_f9) %>%                    # remove other measures
  
  # Rename for Mplus character limit
  rename(naraComp = nara_comp_raw_f9,
         naraAcc = nara_acc_raw_f9,
         woldComp = wold_comp_raw_f8,
         f8age = age_m_f8,
         f9age = age_m_f9) %>% 
  
  # Change ID to a factor
  mutate(yp_id = as.factor(yp_id))
                
```

# Class enumeration

**Aim: to fit models of different n classes, at range of different model specifications.**

Indicator variances can always differ from each other within a class, but different model specifications vary in whether they are separately estimated between
classes. Labels for different specifications taken from Pastor (2007) and Masyn (2013).

## Base model

### Set-up

*Note: Models were edited to increase starts if best log-likelihood value not replicated. Initial plan to increase to 2000 200 (Lubke & Luningham, 2017), but
increased further where necessary.*

Create base model with basic set-up, including age covariates. Specify other model elements for use in different models.

```{r lpa-base}
# Create base model
m_lpa_base <- mplusObject(
  TITLE = "Latent Profile Analysis;",
  ANALYSIS = "estimator = mlr; type = mixture; starts = 500 50; 
              processors = 4(STARTS);",
  VARIABLE = "idvariable = cidB3153; classes = c(1);",
  MODEL = "%OVERALL% 
           f8age; f9age;
           combAcc naraAcc naraComp on f9age;
           woldcomp on f8age;", 
  OUTPUT = "TECH1 TECH8;",
  PLOT = "TYPE = PLOT3;",
  usevariables = colnames(sv_data[,!names(sv_data) %in% c("yp_id")]),
  rdata = sv_data)

# Specify all variances
lpa_vars <- "naraComp; naraAcc; combAcc; woldComp;"

# Specify all covariances 
lpa_covars <- "
  naraComp with naraAcc;
  naraComp with combAcc; 
  naraComp with woldComp; 
  naraAcc with combAcc; 
  naraAcc with woldComp; 
  combAcc with woldComp;  "

# Specify no covariances 
no_covars <- "
  naraComp with naraAcc@0;
  naraComp with combAcc@0; 
  naraComp with woldComp@0; 
  naraAcc with combAcc@0; 
  naraAcc with woldComp@0; 
  combAcc with woldComp@0;  "
```

### Benchmark

Follow class-enumeration process set out by Masyn (2013). Use a one-class LPA (incorporating sample means and covariances) as an absolute fit benchmark.

```{r lpa-benchmark, warning = FALSE}
# Specify benchmark model
m_lpa_benchmark <- update(m_lpa_base,
     TITLE = ~ "LPA Benchmark",
     VARIABLE = ~ "idvariable = cidB3153; classes = c(1);",
     MODEL = as.formula(sprintf("~ . + '%%OVERALL%% \n %s \n %s'", lpa_vars, lpa_covars)))

# Run benchmark model
# mplusModeler(m_lpa_benchmark, "./mplus_models/simpview/lpa_comp/sv_lpa_mbench.dat", run = TRUE)

# Extract parameters for the benchmark model
lpa_benchmark <- readModels(target = "./mplus_models/simpview/lpa_comp", filefilter = "mbench") %>% 
  mixtureSummaryTable(keepCols = c("Parameters", "Observations", "LL", "BIC")) %>% 
  mutate(CAIC = -2*LL + Parameters*(log(Observations) + 1),
           AWE = -2*LL + Parameters*(log(Observations) + 1.5)) %>% 
  select(LL, BIC, CAIC, AWE) %>%
  pivot_longer(LL:AWE, names_to = "statistic", values_to = "benchmark") %>% 
  mutate(statistic_relevel = factor(statistic, levels = c("LL", "BIC", "CAIC", "AWE")))
```

## Model A: Class-invariant, diagonal

**Indicator variances differ from each other within a class, but are constrained to be equal across classes.** **No covariances are estimated.**

### Specification

**Steps 1-3)** Fit series of increasing *k*-class models.

*Note: Started by fitting up to 6-class models, but increased the number of classes after adjusted LRTs implied they might improve model fit. To be thorough, we
ran preliminary models up to 10-classes (the maximum specified in our pre-registration). However, 9- and 10-class models were not generally good fits across
model types (and often did not converge on a single solution), and are not presented in the manuscript.*

```{r lpa-a-fit}
# Fit initial models
m_lpa_a <- lapply(1:10, function(k) {
   body <- update(m_lpa_base,
     TITLE = as.formula(sprintf("~ 'LPA Model A: %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ 'idvariable = cidb3153; classes = c(%d);'", k)),
     MODEL = as.formula(sprintf("~ . + '%s %s'", lpa_vars, no_covars)))

   mplusModeler(body, sprintf("mplus_models/simpview/lpa_comp/sv_lpa_a_%dclass.dat", k), run = FALSE)
 })


# CODE FOR CHECKING MODELS AND AMENDING STARTS FOR MODELS THAT DID NOT REPLICATE BEST LL VALUE
# # Read in output
# lpa_a_out <- readModels(target = "./mplus_models/simpview/lpa_comp", filefilter = "sv_lpa_a")
#
# # Check warnings
# for (model in 1:length(lpa_a_out)){
#   print(lpa_a_out[[model]]$input$title)
#   print(lpa_a_out[[model]]$warnings)
# }

# ## >> Rerun non-replicated models with increased starts
m_lpa_a_starts <- lapply(c(8), function(k) {
   body <- update(m_lpa_base,
     TITLE = as.formula(sprintf("~ 'LPA Model A: %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ 'idvariable = cidb3153; classes = c(%d);'", k)),
     MODEL = as.formula(sprintf("~ . + '%s %s'", lpa_vars, no_covars)),
     ANALYSIS = ~ "estimator = mlr; type = mixture; starts = 10000 2500;
              processors = 4(STARTS);")

   mplusModeler(body, sprintf("mplus_models/simpview/lpa_comp/sv_lpa_a_%dclass.dat", k), run = TRUE)
})
```

### Indices

**Steps 4-5)** Extract fit indices, and use to select smaller subset of candidate models.

```{r lpa-a-output, warning = FALSE}
# Read in output
lpa_a_out <- readModels(target = "./mplus_models/simpview/lpa_comp", filefilter = "(sv_lpa_a_)([1-9])")

# Print table 
lpa_a_summary <- lpa_enum_table(output = lpa_a_out)
lpa_a_summary %>% kable("pipe")

# Elbow plots for information criteria
lpa_enum_elbow(lpa_a_summary, benchmark_stats = lpa_benchmark)
```

```{r test, include = FALSE}
lpa_a_lmr <- recomp_LMR(orig_mods = m_lpa_a, 
                        orig_output = lpa_a_out, 
                        mods = c(1:length(lpa_a_out)),
                        kstarts = "500 50",
                        filepath = "mplus_models/simpview/lpa_comp",
                        rerun = FALSE
                        )

lpa_enum_table(lpa_a_lmr) %>% 
  select(Specification, LMR_check)

lpa_a_lmr <- recomp_LMR(orig_mods = m_lpa_a, 
                        orig_output = lpa_a_out, 
                        mods = c(8, 9),
                        kstarts = "500 50",
                        filepath = "mplus_models/simpview/lpa_comp"
                        )

lpa_enum_table(lpa_a_lmr, LMR_warn = TRUE) %>% 
  select(Specification, LMR_check)

```

Best model for:

| **a) Absolute fit** (fewest classes with better LL than benchmark)**:** *5-class*
| **b) Information heuristics** (diminishing gains from elbow plots)**:** *5-class*
| **c) Adjusted LRTs** (fewest classes not sig improved by additional classes)**:** *8-class*
| **d) Approximate BF** (fewest classes with moderate-strong evidence compared to previous model): *10-class/NA*
| **e) Approximate correct model probability:** *10-class*

### Inspection

**Step 6)** View classification diagnostics for candidate models. Can also inspect how the classes correspond across models.

```{r lpa-a-candidates, warning = FALSE, message = FALSE}
# Extract participant level data
lpa_a_rerun <- mm_extract_data(orig_mods = m_lpa_a, orig_output = lpa_a_out, 
                               candidate_mods = c(5:8),                      
                               filepath = "mplus_models/simpview/lpa_comp",
                               rerun = FALSE, optseed = TRUE,          
                               one_fit = TRUE)

# Print table, and append bLRT values to main output 
# lpa_enum_table(lpa_a_rerun)
lpa_a_summary <- add_bLRT(lpa_a_rerun, lpa_a_summary)
lpa_a_summary %>% kable("pipe")

# Compute classification diagnostics
class_diag(lpa_a_rerun)

# Plot class means
plotMixtures_simpView(lpa_a_rerun[1:2])

# Inspect transitions 
extract_classes(lpa_a_rerun[1:2], type = "lpa")
```

**Comments:**

-   **Classification diagnostics:** *Average posterior class probability fine for 5- and 6-class models, but poor beyond.*
-   **Interpretability:** *No clear differences between 5- and 6-class models; additional class looks largely to add further dissociate between ability. Simpler
    model preferred.*

### Selection

**Step 7)** Select final model in class enumeration process, for model specification A.

```{r lpa-a-selected}
lpa_a_final_nclass <- 5            
lpa_a_final_m <- lpa_a_rerun[1]    
```

## Model B: Class-invariant, unrestricted

**Indicator variances differ from each other within a class and can covary, but variances and covariances constrained to be equal across classes.**

### Specification

**Steps 1-3)** Fit series of increasing *k*-class models.

```{r lpa-b-fit}
m_lpa_b <- lapply(1:10, function(k) {
   body <- update(m_lpa_base,
     TITLE = as.formula(sprintf("~ 'LPA Model B: %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ 'idvariable = cidB3153; classes = c(%d);'", k)),
     
     # Update model spec 1: overall model estimates (variances and covariances)
     MODEL = as.formula(sprintf("~ . + '\n %s \n %s'", lpa_vars, lpa_covars)))
   
   # Run model
   mplusModeler(body, sprintf("mplus_models/simpview/lpa_comp/sv_lpa_b_%dclass.dat", k), run = FALSE)
 })

# CODE FOR CHECKING MODELS AND AMENDING STARTS FOR MODELS THAT DID NOT REPLICATE BEST LL VALUE
# # Read in output
# lpa_b_out <- readModels(target = "./mplus_models/simpview/lpa_comp", filefilter = "sv_lpa_b")
# 
# # Check warnings
# for (model in 1:length(lpa_b_out)){
#   print(lpa_b_out[[model]]$input$title)
#   print(lpa_b_out[[model]]$warnings)
# }
# 
# ## >> Rerun  model with increased starts
# m_lpa_b_starts <- lapply(c(6, 8, 9), function(k) {
#    body <- update(m_lpa_base,
#      TITLE = as.formula(sprintf("~ 'LPA Model B: %d classes;'", k)),
#      VARIABLE = as.formula(sprintf("~ 'idvariable = cidB3153; classes = c(%d);'", k)),
# 
#      # Update model spec 1: overall model estimates (variances and covariances)
#      MODEL = as.formula(sprintf("~ . + '\n %s \n %s'", lpa_vars, lpa_covars)),
# 
#      ANALYSIS = ~ "estimator = mlr; type = mixture; starts = 6000 1500;
#               processors = 4(STARTS);")
# 
#    # Run model
#    mplusModeler(body, sprintf("mplus_models/simpview/lpa_comp/sv_lpa_b_%dclass.dat", k), run = FALSE)
# })
```

### Indices

**Steps 4-5)** Extract fit indices, and use to select smaller subset of candidate models.

```{r lpa-b-output, warning = FALSE, message = FALSE}
# Read in output
lpa_b_out <- readModels(target = "./mplus_models/simpview/lpa_comp", filefilter = "(sv_lpa_b_)([1-9])")

# Print table 
lpa_b_summary <- lpa_enum_table(output = lpa_b_out)
lpa_b_summary[c(6, 8, 9), c(3, 5:11)] <- NA  # Remove values from non-trusted models 
lpa_b_summary %>% kable("pipe")

# Elbow plots for information criteria
lpa_enum_elbow(lpa_b_summary, benchmark_stats = lpa_benchmark)
```

```{r test, include = FALSE}
lpa_b_lmr <- recomp_LMR(orig_mods = m_lpa_b, 
                        orig_output = lpa_b_out, 
                        mods = c(1:length(lpa_b_out)),
                        #kstarts = "500 50",
                        filepath = "mplus_models/simpview/lpa_comp",
                        rerun = FALSE)

lpa_b_lmr <- recomp_LMR(orig_mods = m_lpa_b, 
                        orig_output = lpa_b_out, 
                        mods = c(5),
                        kstarts = "10000 2500",
                        filepath = "mplus_models/simpview/lpa_comp"
                        )

lpa_enum_table(lpa_b_lmr, LMR_warn = TRUE) %>% 
  select(Specification, LMR_check)

### don't need further running, as problematic models already flagged as problematic from above
```

Best model for:

| **a) Absolute fit** (fewest classes with better LL than benchmark)**:** *2-class*
| **b) Information heuristics** (diminishing gains from elbow plots)**:** *2-class/5-class*
| **c) Adjusted LRTs** (fewest classes not sig improved by additional classes)**:** *5-class*
| **d) Approximate BF** (fewest classes with moderate-strong evidence compared to next model): *5-class*
| **e) Approximate correct model probability:** *?*

### Inspection

**Step 6)** View classification diagnostics for candidate models.

```{r lpa-b-candidates, warning = FALSE, message = FALSE}
# Extract participant level data
lpa_b_rerun <- mm_extract_data(orig_mods = m_lpa_b, orig_output = lpa_b_out, 
                               candidate_mods = c(4:5),                           # change models to best candidates
                               filepath = "mplus_models/simpview/lpa_comp",
                               rerun = FALSE, optseed = TRUE,          
                               one_fit = TRUE)

# Print table, and append bLRT values to main output 
# lpa_enum_table(lpa_b_rerun)
lpa_b_summary <- add_bLRT(lpa_b_rerun, lpa_b_summary)
lpa_b_summary %>% kable("pipe")

# Compute classification diagnostics
class_diag(lpa_b_rerun)

# Plot class means
plotMixtures_simpView(lpa_b_rerun)

# Inspect transitions 
extract_classes(lpa_b_rerun, type = "lpa")
```

Comments:

-   **Classification diagnostics:** *Entropy and class separation adequate for both models.*
-   **Interpretability:** *5-class model splits out a lower performing group (this is also the model best-supported by the adjusted LRTs).*

### Selection

**Step 7)** Select final model in class enumeration process, for model specification B.

```{r lpa-b-selected}
lpa_b_final_nclass <- 5           
lpa_b_final_m <- lpa_b_rerun[2]   
```

## Model C: Class-varying, diagonal

**Indicator variances are estimated separately for each class.** **No covariances are estimated.**

### Specification

**Steps 1-3)** Fit series of increasing *k*-class models.

```{r lpa-c-fit}
m_lpa_c <- lapply(1:10, function(k) {
  
  # Update model spec 1: overall model estimates (variances only)
  body <- update(m_lpa_base,
              MODEL = as.formula(sprintf("~ . + '%s %s'", lpa_vars, no_covars)))
  
  # Create class-level specifications (to include variances only)
  class_spec <- paste0("%c#1% \n ", lpa_vars)
  if (k > 1){
    for (i in 2:k){
      class_spec <- paste0(class_spec, "\n %c#", i, "% \n ", lpa_vars)
    }
  }
  
  # ...Update model spec 2: class estimates
  body <- update(body,
     TITLE = as.formula(sprintf("~ 'LPA Model C: %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ 'idvariable = cidb3153; classes = c(%d);'", k)),
     MODEL = as.formula(sprintf("~ . + '%s'", class_spec)))
   
  # Run model
  mplusModeler(body, sprintf("mplus_models/simpview/lpa_comp/sv_lpa_c_%dclass.dat", k), run = FALSE)
 })

# CODE FOR CHECKING MODELS AND AMENDING STARTS FOR MODELS THAT DID NOT REPLICATE BEST LL VALUE
# # Read in output
# lpa_c_out <- readModels(target = "./mplus_models/simpview/lpa_comp", filefilter = "sv_lpa_c")
# 
# # Check warnings
# for (model in 1:length(lpa_c_out)){
#   print(lpa_c_out[[model]]$input$title)
#   print(lpa_c_out[[model]]$warnings)
# }

# ### >> Rerun with increased starts
m_lpa_c_starts <- lapply(c(8), function(k) {

  # Update model spec 1: overall model estimates (variances only)
  body <- update(m_lpa_base,
              MODEL = as.formula(sprintf("~ . + '%s %s'", lpa_vars, no_covars)))

  # Create class-level specifications (to include variances only)
  class_spec <- paste0("%c#1% \n ", lpa_vars)
  if (k > 1){
    for (i in 2:k){
      class_spec <- paste0(class_spec, "\n %c#", i, "% \n ", lpa_vars)
    }
  }

  # ...Update model spec 2: class estimates
  body <- update(body,
     TITLE = as.formula(sprintf("~ 'LPA Model C: %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ 'idvariable = cidb3153; classes = c(%d);'", k)),
     MODEL = as.formula(sprintf("~ . + '%s'", class_spec)),
     ANALYSIS = ~ "estimator = mlr; type = mixture; starts = 8000 2000;
              processors = 4(STARTS);")

  # Run model
  mplusModeler(body, sprintf("mplus_models/simpview/lpa_comp/sv_lpa_c_%dclass.dat", k), run = TRUE)
 })
```

### Indices

**Steps 4-5)** Extract fit indices, and use to select smaller subset of candidate models.

```{r lpa-c-output, warning = FALSE, message = FALSE}
# Read in output
lpa_c_out <- readModels(target = "./mplus_models/simpview/lpa_comp", filefilter = "(sv_lpa_c_)([1-9])")

# Print table 
lpa_c_summary <- lpa_enum_table(output = lpa_c_out) 
lpa_c_summary[c(9), c(3, 5:11)] <- NA  # Remove values from non-trusted models 
lpa_c_summary %>% kable("pipe")

# Elbow plots for information criteria
lpa_enum_elbow(lpa_c_summary, benchmark_stats = lpa_benchmark)
```

```{r test, include = FALSE}
lpa_c_lmr <- recomp_LMR(orig_mods = m_lpa_c, 
                        orig_output = lpa_c_out, 
                        mods = c(1:length(lpa_c_out)),
                        #kstarts = "500 50",
                        filepath = "mplus_models/simpview/lpa_comp",
                        rerun = FALSE
                        )

lpa_c_lmr <- recomp_LMR(orig_mods = m_lpa_c, 
                        orig_output = lpa_c_out, 
                        mods = c(8,9),
                        kstarts = "1000 250",
                        filepath = "mplus_models/simpview/lpa_comp"
                        )

lpa_enum_table(lpa_c_lmr, LMR_warn = TRUE) %>% 
  select(Specification, LMR_check)

### GOT TO HERE, NEED TO RE-RUN K7
```

| Best model for:Â  **a) Absolute fit** (fewest classes with better LL than benchmark)**:** *5-class* **b) Information heuristics** (diminishing gains from elbow plots)**:** *5-class* **c) Adjusted LRTs** (fewest classes not sig improved by additional classes)**:** *7-class* **d) Approximate BF** (fewest classes with moderate-strong evidence compared to next model): *10-class/NA* **e) Approximate correct model probability:** *10-class*

### Inspection

**Step 6)** View classification diagnostics for candidate models.

```{r lpa-c-candidates, warning = FALSE, message = FALSE}
# Extract participant level data
lpa_c_rerun <- mm_extract_data(orig_mods = m_lpa_c, orig_output = lpa_c_out, 
                               candidate_mods = c(5:7),                          
                               filepath = "mplus_models/simpview/lpa_comp",
                               rerun = FALSE, optseed = TRUE,          
                               one_fit = TRUE)

# Print table, and append bLRT values to main output 
#lpa_enum_table(lpa_c_rerun)
lpa_c_summary <- add_bLRT(lpa_c_rerun, lpa_c_summary)
lpa_c_summary %>% kable("pipe")

# Compute classification diagnostics
class_diag(lpa_c_rerun)

# Plot class means
plotMixtures_simpView(lpa_c_rerun)

# Inspect transitions 
extract_classes(lpa_c_rerun, type = "lpa")
```

Comments:

-   **Classification diagnostics:** *All classification diagnostics adequate*
-   **Interpretability:** *Generally dissociated by ability; 7-class model further dissociates at higher level of ability. Simpler model preferred.*

### Selection

**Step 7)** Select final model in class enumeration process, for model specification C.

```{r lpa-c-selected}
lpa_c_final_nclass <- 5         
lpa_c_final_m <- lpa_c_rerun[1]
```

## Model D: Class-varying (part), unrestricted

**Indicator variances are estimated separately for each class.**

**Covariances are constrained to be equal across classes.**

### Specification

**Steps 1-3)** Fit series of increasing *k*-class models.

```{r lpa-d-fit}
m_lpa_d <- lapply(1:10, function(k) {
  
  # Update model spec 1: overall model estimates (variances and covariances)
  body <- update(m_lpa_base,
              MODEL = as.formula(sprintf("~ . + '%s \n %s '", lpa_vars, lpa_covars)))   
  
  # Create class-level specifications (variances only)
  class_spec <- paste0("%c#1% \n ", lpa_vars)
  if (k > 1){
    for (i in 2:k){
      class_spec <- paste0(class_spec, "\n %c#", i, "% \n ", lpa_vars)
    }
  }
  
  # ...Update model spec 2: class estimates
  body <- update(body,
     TITLE = as.formula(sprintf("~ 'LPA Model D: %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ 'idvariable = cidb3153; classes = c(%d);'", k)),
     MODEL = as.formula(sprintf("~ . + '%s'", class_spec)))
  
  # Run model
  mplusModeler(body, sprintf("mplus_models/simpview/lpa_comp/sv_lpa_d_%dclass.dat", k), run = FALSE)
 })

# # Read in output
# lpa_d_out <- readModels(target = "./mplus_models/simpview/lpa_comp", filefilter = "sv_lpa_d")
# 
# # Check warnings
# for (model in 1:length(lpa_d_out)){
#   print(lpa_d_out[[model]]$input$title)
#   print(lpa_d_out[[model]]$warnings)
# }
# 
# # Rerun models with local maxima issues
# m_lpa_d_rerun <- lapply(c(5), function(k) {
#   
#   # Update model spec 1: overall model estimates (variances and covariances)
#   body <- update(m_lpa_base,
#               MODEL = as.formula(sprintf("~ . + '%s \n %s '", lpa_vars, lpa_covars)))   
#   
#   # Create class-level specifications (variances only)
#   class_spec <- paste0("%c#1% \n ", lpa_vars)
#   if (k > 1){
#     for (i in 2:k){
#       class_spec <- paste0(class_spec, "\n %c#", i, "% \n ", lpa_vars)
#     }
#   }
#   
#   # ...Update model spec 2: class estimates
#   body <- update(body,
#      TITLE = as.formula(sprintf("~ 'LPA Model D: %d classes;'", k)),
#      VARIABLE = as.formula(sprintf("~ 'idvariable = cidb3153; classes = c(%d);'", k)),
#      MODEL = as.formula(sprintf("~ . + '%s'", class_spec)),
#      ANALYSIS = ~ "estimator = mlr; type = mixture; starts = 4000 1000;
#               processors = 4(STARTS);")
#   
#   # Run model
#   mplusModeler(body, sprintf("mplus_models/simpview/lpa_comp/sv_lpa_d_%dclass.dat", k), run = TRUE)
#  })
```

### Indices

**Steps 4-5)** Extract fit indices, and use to select smaller subset of candidate models.

```{r lpa-d-output, warning = FALSE, message = FALSE}
# Read in output
lpa_d_out <- readModels(target = "./mplus_models/simpview/lpa_comp", filefilter = "(sv_lpa_d_)([1-9])")

# Print table 
lpa_d_summary <- lpa_enum_table(output = lpa_d_out)
lpa_d_summary[c(7, 9, 10), c(3, 5:11)] <- NA  # Could also remove values from non-trusted models?
lpa_d_summary %>% kable("pipe")

# Elbow plots for information criteria
lpa_enum_elbow(lpa_d_summary, benchmark_stats = lpa_benchmark)
```

Best model for:

| **a) Absolute fit** (fewest classes with better LL than benchmark)**:** *2-class*
| **b) Information heuristics** (diminishing gains from elbow plots)**:** *3-class*
| **c) Adjusted LRTs** (fewest classes not sig improved by additional classes)**:** *3-class*
| **d) Approximate BF** (fewest classes with moderate-strong evidence compared to next model): *NA*
| **e) Approximate correct model probability:** *NA*

### Inspection

**Step 6)** View classification diagnostics for candidate models.

```{r lpa-d-candidates, warning = FALSE, message = FALSE}
# Extract participant level data
lpa_d_rerun <- mm_extract_data(orig_mods = m_lpa_d, orig_output = lpa_d_out, 
                               candidate_mods = c(3:5),                          
                               filepath = "mplus_models/simpview/lpa_comp",
                               rerun = FALSE, optseed = TRUE,          
                               one_fit = TRUE)

# Print table, and append bLRT values to main output 
#lpa_enum_table(lpa_d_rerun)
lpa_d_summary <- add_bLRT(lpa_d_rerun, lpa_d_summary)
lpa_d_summary %>% kable("pipe")

# Compute classification diagnostics
class_diag(lpa_d_rerun)

# Plot class means
plotMixtures_simpView(lpa_d_rerun)

# Inspect transitions 
extract_classes(lpa_d_rerun, type = "lpa")
```

Comments:

-   **Classification diagnostics:** *Average posterior class probability similar for first two models, getting lower for 5-class model*
-   **Interpretability:** *4-class model better dissociates higher levels of ability, but information heuristics and LL suggest little improvement between 3-
    and 4- classes, simpler model preferred*

### Selection

**Step 7)** Select final model in class enumeration process, for model specification D.

```{r lpa-d-selected}
lpa_d_final_nclass <- 3            
lpa_d_final_m <- lpa_d_rerun[1]   
```

## Model E: Class-varying (full), unrestricted

**All variances and covariances can vary between clusters.**

### Specification

**Steps 1-3)** Fit series of increasing *k*-class models.

```{r lpa-e-fit}
m_lpa_e <- lapply(1:10, function(k) {
  
  # Update model spec 1: overall model estimates (variances and covariances)
  body <- update(m_lpa_base,
              MODEL = as.formula(sprintf("~ . + '%s \n %s '", lpa_vars, lpa_covars)))   
  
  # Create class-level specifications (variances and covariances)
  class_spec <- paste0("%c#1% \n ", lpa_vars,  lpa_covars)
  if (k > 1){
    for (i in 2:k){
      class_spec <- paste0(class_spec, "\n %c#", i, "% \n ", lpa_vars, lpa_covars)
    }
  }
  
  # ...Update model spec 2: class estimates
  body <- update(body,
     TITLE = as.formula(sprintf("~ 'LPA Model E: %d classes;'", k)),
     VARIABLE = as.formula(sprintf("~ 'idvariable = cidb3153; classes = c(%d);'", k)),
     MODEL = as.formula(sprintf("~ . + '%s'", class_spec)))

  # Run model
  mplusModeler(body, sprintf("mplus_models/simpview/lpa_comp/sv_lpa_e_%dclass.dat", k), run = FALSE)
 })

# CODE FOR CHECKING MODELS AND AMENDING STARTS FOR MODELS THAT DID NOT REPLICATE BEST LL VALUE
# # Read in output
# lpa_e_out <- readModels(target = "./mplus_models/simpview/lpa_comp", filefilter = "sv_lpa_e")
# 
# # Check warnings
# for (model in 1:length(lpa_e_out)){
#   print(lpa_e_out[[model]]$input$title)
#   print(lpa_e_out[[model]]$warnings)
# }
# 
# # >> Re-fit models that did not replicate solution
# m_lpa_e_starts <- lapply(c(2), function(k) {
# 
#   # Update model spec 1: overall model estimates (variances and covariances)
#   body <- update(m_lpa_base,
#               MODEL = as.formula(sprintf("~ . + '%s \n %s '", lpa_vars, lpa_covars)))
# 
#   # Create class-level specifications (variances and covariances)
#   class_spec <- paste0("%c#1% \n ", lpa_vars,  lpa_covars)
#   if (k > 1){
#     for (i in 2:k){
#       class_spec <- paste0(class_spec, "\n %c#", i, "% \n ", lpa_vars, lpa_covars)
#     }
#   }
# 
#   # ...Update model spec 2: class estimates
#   body <- update(body,
#      TITLE = as.formula(sprintf("~ 'LPA Model E: %d classes;'", k)),
#      VARIABLE = as.formula(sprintf("~ 'idvariable = cidb3153; classes = c(%d);'", k)),
#      MODEL = as.formula(sprintf("~ . + '%s'", class_spec)),
#      ANALYSIS = ~ "estimator = mlr; type = mixture; starts = 6000 1500;
#               processors = 4(STARTS);")
# 
#   # Run model
#   mplusModeler(body, sprintf("mplus_models/simpview/lpa_comp/sv_lpa_e_%dclass.dat", k), run = FALSE)
# })
```

### Indices

**Steps 4-5)** Extract fit indices, and use to select smaller subset of candidate models.

```{r lpa-e-output, warning = FALSE, message = FALSE}
# Read in output
lpa_e_out <- readModels(target = "./mplus_models/simpview/lpa_comp", filefilter = "(sv_lpa_e_)([1-9])")

# Print table 
lpa_e_summary <- lpa_enum_table(output = lpa_e_out) 
lpa_e_summary[c(10), c(3, 5:11)] <- NA  # Remove values from non-trusted models
lpa_e_summary

# Elbow plots for information criteria
lpa_enum_elbow(lpa_e_summary, benchmark_stats = lpa_benchmark)

ggsave("../output/figures/lpa_e_elbow.png")
```

Best model for:

| **a) Absolute fit** (fewest classes with better LL than benchmark)**:** *2-class*
| **b) Information heuristics** (diminishing gains from elbow plots)**:** *3- or 4-class*
| **c) Adjusted LRTs** (fewest classes not sig improved by additional classes)**:** *4-class*
| **d) Approximate BF** (fewest classes with moderate-strong evidence compared to next model): *4-class*
| **e) Approximate correct model probability:** *8-class*

### Inspection

**Step 6)** View classification diagnostics for candidate models.

```{r lpa-e-candidates, warning = FALSE, message = FALSE}
# Extract participant level data
lpa_e_rerun <- mm_extract_data(orig_mods = m_lpa_e, orig_output = lpa_e_out, 
                               candidate_mods = c(3:4),                          
                               filepath = "mplus_models/simpview/lpa_comp",
                               rerun = FALSE, optseed = TRUE,          
                               one_fit = TRUE)

# Print table, and append bLRT values to main output 
#lpa_enum_table(lpa_e_rerun)
lpa_e_summary <- add_bLRT(lpa_e_rerun, lpa_e_summary)
lpa_e_summary %>% kable("pipe")

# Compute classification diagnostics
class_diag(lpa_e_rerun)

# Plot class means
plotMixtures_simpView(lpa_e_rerun)

# Inspect transitions 
extract_classes(lpa_e_rerun, type = "lpa")
```

Comments:

-   **Classification diagnostics:** *Lowest average posterior class assignment prob similar across models*
-   **Interpretability:** *Further classes do not look well dissociated (e.g., 5-class model - splits average performance into 2 small overlapping classes)*

### Selection

**Step 7)** Select final model in class enumeration process, for model specification E.

```{r lpa-e-selected}
lpa_e_final_nclass <- 4          
lpa_e_final_m <- lpa_e_rerun[2]   
```

# Best models

**Aim: Inspect the 5 best candidate models and select the best overall model**

## Final candidates

Use the 5 best candidate models identified above to recalculate the approximate correct model probability, and re-inspect the classification diagnostics of
each.

```{r final-candidates, message = FALSE, warning = FALSE}
# Select best models
best_fits <- c(lpa_a_final_m, lpa_b_final_m, lpa_c_final_m, lpa_d_final_m, lpa_e_final_m)

# Compute cMP for each
best_summaries <- lpa_enum_table(best_fits) %>% 
  select(-c(VLMR_p, LMR_p, BF)) %>% 
  rename(cmP_best = cmP_k) %>% 
  arrange(Specification)
best_summaries %>% kable("pipe")

best_cmP <- best_summaries %>% 
  select(Specification, Classes, cmP_best)
  
# Compute classification diagnostics
class_diag(best_fits)

# Plot class means
plotMixtures_simpView(best_fits)
```

## All model summaries

Combine all model fit indices into a single table, highlighting the best candidate models.

```{r lpa-all-table}
# Combine all summary tables
lpa_all_summaries <- rbind(lpa_a_summary, lpa_b_summary, lpa_c_summary, 
                           lpa_d_summary, lpa_e_summary) %>% 
  filter(Classes <= 8)

# Extract number of classes modelled in class enumeration process
k_classes_modelled <- max(lpa_all_summaries$Classes, na.rm = TRUE)

# Specify table row for selected models for each spec
a_row <- lpa_a_final_nclass
b_row <- (1*k_classes_modelled) + lpa_b_final_nclass
c_row <- (2*k_classes_modelled) + lpa_c_final_nclass
d_row <- (3*k_classes_modelled) + lpa_d_final_nclass
e_row <- (4*k_classes_modelled) + lpa_e_final_nclass

# Format table, highlight class model of best fit in each specification
lpa_all_summaries %>% 
  left_join(best_cmP, by = c("Specification", "Classes")) %>% 
  mutate(Specification = " ") %>%  
  mutate(across(where(is.numeric), round, 2)) %>% 
  mutate(VLMR_p = ifelse(VLMR_p == 0, "<0.01", VLMR_p),
         LMR_p = ifelse(LMR_p == 0, "<0.01", LMR_p),
         bLRT_p = ifelse(bLRT_p == 0, "<0.01", bLRT_p),
         BF = ifelse(BF == 0.00, "<0.01",
                     ifelse(BF > 100, ">100", BF)),
         cmP_k = ifelse(cmP_k == 0, "<0.01",
                      ifelse(cmP_k == 1, ">0.99", cmP_k))) %>%
  replace(is.na(.), "-") %>% 
  kbl(align = c("l", "c", "r", "c", "r", "r","r","r","r","r","r","r")) %>% 
  kable_classic(html_font = "Cambria") %>% 
  pack_rows(index = c("Model A" = k_classes_modelled, "Model B" = k_classes_modelled, "Model C" = k_classes_modelled, "Model D" = k_classes_modelled, "Model E" = k_classes_modelled)) %>%
  row_spec(1, align = "c") %>% 
  row_spec(9, background = "#F0F0F0") %>% 
  row_spec(c(a_row, b_row, c_row, d_row, e_row), bold = T)  # edit for row numbers of selected models within each model spec 

# Save table as pdf for later viewing
# save_kable(table_all, file = "../output/tables/SimpView_lpa_all_modelfit.pdf")
```

```{r lpa-all-table-save, include = FALSE}
table_all <- 
  lpa_all_summaries %>% 
  left_join(best_cmP, by = c("Specification", "Classes")) %>% 
  mutate(Specification = substr(Specification, 5, 12)) %>% 
  mutate(across(where(is.numeric), round, 2)) %>% 
  mutate(VLMR_p = ifelse(VLMR_p == 0, "<0.01", VLMR_p),
         LMR_p = ifelse(LMR_p == 0, "<0.01", LMR_p),
         bLRT_p = ifelse(bLRT_p == 0, "<0.01", bLRT_p),
         BF = ifelse(BF == 0.00, "<0.01",
                     ifelse(BF > 100, ">100", BF)),
         cmP_k = ifelse(cmP_k == 0, "<0.01",
                      ifelse(cmP_k == 1, ">0.99", cmP_k))) %>%
  replace(is.na(.), "-")

write.csv(table_all, "../output/tables/lpa_all_indices.csv", row.names = FALSE)
```

Combine all fit indices into a single plot.

```{r lpa-all-elbow}
# Grouped elbow plots
lpa_all_summaries %>%
    mutate(model_spec = substr(Specification, 11, 12)) %>% 
    select(model_spec, Classes, LL, BIC, CAIC, AWE) %>%
    pivot_longer(LL:AWE, names_to = "statistic", values_to = "value") %>%
    mutate(statistic_relevel = factor(statistic, levels = c("LL", "BIC", "CAIC", "AWE"))) %>% 
    ggplot(aes(x = as.factor(Classes), y = value)) +
    geom_point(aes(shape = model_spec)) +
    geom_line(aes(group = model_spec, linetype = model_spec)) +
    xlab("k classes") +
    theme_bw() +
    theme(panel.grid.major.x = element_blank(),
          panel.grid.minor.y = element_blank()) +
    geom_hline(data = lpa_benchmark, aes(yintercept = benchmark), linetype = "dashed", colour = "red") +
    facet_wrap(statistic_relevel ~., scales = "free") + 
    labs(shape = "Model spec:", linetype = "Model spec:")

# save out for easier viewing
ggsave("../output/figures/SimpView_lpa_all_indices.png", dpi = 600, width = 7, height = 4, units = "in")
```

## Final model

*Models A and C (no covariances) showed much poorer fit to the data than other model specifications. This makes theoretical sense given that we include multiple
measures of reading and comprehension skills.*

*Of the best candidate models, we selected Model E4 as the final model. It shows slightly better fit to the data according to all fit indices. It also appears
to better dissociate high and low performers than the other possible models (B5 and D3). However, we also noted that entropy was lower.*

*Overall, none of the candidate models inspected during class enumeration or final model selection appeared to dissociate relative strengths/weaknesses in
accuracy and comprehension skills, as we had anticipated. That is, all models estimated classes based on overall performance/ability across all measures. Thus,
we are reassured that our conclusions are not affected by our decision over the final best model.*

```{r final-model}
readModels("mplus_models/simpview/lpa_comp/sv_lpa_e_rerun_4class.out", what="parameters")$parameters$r2 %>% kable("pipe")
readModels("mplus_models/simpview/lpa_comp/sv_lpa_e_rerun_4class.out", what="parameters")$parameters$stdyx.standardized %>% kable("pipe")
```

This model will be taken forward for cross-validation with subsample B (WP1_SimpView_CrossValidation.Rmd).

# Version info {#version}

*Package versions were up-to-date as of 12/08/2021.* *LPA models were run using Mplus Version 8.5.*

```{r version}
sessionInfo()
```
