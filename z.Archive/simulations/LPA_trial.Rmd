---
title: "LPA with simulated reading data"
output: 
  html_document:
    toc: true
    toc_float: true
---

This script is a first attempt at using the tidyLPA package to perform a latent profile analysis. 
It uses simulated data, designed around the variables available in ALSPAC for the first analysis (Simple View).  

Package vignette: [https://data-edu.github.io/tidyLPA/articles/Introduction_to_tidyLPA.html](https://data-edu.github.io/tidyLPA/articles/Introduction_to_tidyLPA.html).See also [this tutorial](https://rstudio-pubs-static.s3.amazonaws.com/553733_4d23ff5746ec4c8584969ce0768d6db5.html) for some further comments. 

```{r libraries, message = FALSE, warning = FALSE}
library(SIN)
library(MASS)
library(psych)
library(naniar)
library(tidyverse)
library(tidyLPA)
library(GGally)
```

```{r set-up}
options(scipen = 999)
set.seed(20201105)
```

# Simulate data

Create dataset similar to one that we will use - three reading accuracy measures, a reading comprehension measure and listening comprehension.  

### Specify variables and relationships

For now, have just used standard score estimates for each task, and made up the correlations. 

```{r measures}

#CLASS 1

################# MEASURES ###################

# NARA - comprehension
nara_comp_m1 <- 105
nara_comp_sd1 <- 5
  
# NARA - accuracy
nara_acc_m1 <- 105
nara_acc_sd1 <- 5

# Word/nonword reading
word_acc_m1 <- 105
word_acc_sd1 <- 5
nonword_acc_m1 <- 105
nonword_acc_sd1 <-5

# WOLD listening
wold_comp_m1 <- 105
wold_comp_sd1 <- 5

#CLASS 2

################# MEASURES ###################

# NARA - comprehension
nara_comp_m2 <- 80
nara_comp_sd2 <- 7
  
# NARA - accuracy
nara_acc_m2 <- 80
nara_acc_sd2 <- 7

# Word/nonword reading
word_acc_m2 <- 80
word_acc_sd2 <- 7
nonword_acc_m2 <- 80
nonword_acc_sd2 <-7

# WOLD listening
wold_comp_m2 <- 80
wold_comp_sd2 <- 7

#CLASS 3

################# MEASURES ###################

# NARA - comprehension
nara_comp_m3 <- 90
nara_comp_sd3 <- 10
  
# NARA - accuracy
nara_acc_m3 <- 90
nara_acc_sd3 <- 10

# Word/nonword reading
word_acc_m3 <- 90
word_acc_sd3 <- 10
nonword_acc_m3 <- 90
nonword_acc_sd3 <-10

# WOLD listening
wold_comp_m3 <- 90
wold_comp_sd3 <- 10


################# CORRELATIONS ###################

#assumption is that we have same correlation among measures for each class

nara_comp_acc <- 0.5
nara_comp_word <- 0.4
nara_comp_nonword <- 0.4
nara_comp_wold <- 0.6
nara_acc_word <- 0.6
nara_acc_nonword <- 0.5
nara_acc_wold <- 0.2
word_acc_nonword <- 0.7
word_acc_wold <- 0.2
nonword_acc_wold <- 0.2
```

*If using raw scores, how can we account for age differences? Regress out of each raw score?*

### Simulate dataset

Compute covariance matrix based on the above estimates, and use to simulate specified number of observations.

```{r simulate-data}
################# COVARIANCES ###################

# List names in order
test_names <- c("nara_comp", "nara_acc", "word_acc", "nonword_acc", "wold_comp")

# List SDs in order above
stddev1 <- c(nara_comp_sd1, nara_acc_sd1, word_acc_sd1, nonword_acc_sd1, wold_comp_sd1)
stddev2 <- c(nara_comp_sd2, nara_acc_sd2, word_acc_sd2, nonword_acc_sd2, wold_comp_sd2)
stddev3 <- c(nara_comp_sd3, nara_acc_sd3, word_acc_sd3, nonword_acc_sd3, wold_comp_sd3)

# Create correlation matrix using above estimates
corr <- matrix(c(1, nara_comp_acc, nara_comp_word, nara_comp_nonword, nara_comp_wold,
                 nara_comp_acc, 1, nara_acc_word, nara_acc_nonword, nara_acc_wold,
                 nara_comp_word, nara_acc_word, 1, word_acc_nonword, word_acc_wold,
                 nara_comp_nonword, nara_acc_nonword, word_acc_nonword, 1, nonword_acc_wold,
                 nara_comp_wold, nara_acc_wold, word_acc_wold, nonword_acc_wold, 1),
               byrow = TRUE, nrow = 5, 
               dimnames = list(test_names, test_names))

# Compute covariance matrix
covar1 <- sdcor2cov(stddev1, corr)
covar2 <- sdcor2cov(stddev2, corr)
covar3 <- sdcor2cov(stddev3, corr)

################# SIMULATE DATA ###################

# List means in order above
means1 <- c(nara_comp_m1, nara_acc_m1, word_acc_m1, nonword_acc_m1, wold_comp_m1)
means2 <- c(nara_comp_m2, nara_acc_m2, word_acc_m2, nonword_acc_m2, wold_comp_m2)
means3 <- c(nara_comp_m3, nara_acc_m3, word_acc_m3, nonword_acc_m3, wold_comp_m3)

# Number of observations
n_ppts <- 1000

#latent profile probabilities.
class_probs<-c(0.6,0.2,0.2)

# generate mixing profile indicator based on profile probabilities
k<-sample(1:3,n_ppts,class_probs,replace=TRUE)

# Set up empty holding dataframe
sim_dat <-data.frame(nara_comp=numeric(), nara_acc=numeric(), word_acc=numeric(), nonword_acc=numeric(), wold_comp=numeric())

#sample from the three multivariate normal distributions in the correct proportions according to profile probabilities.
for(i in 1:n_ppts){
sim_dat[i,]<-switch(k[i],mvrnorm(n = 1, mu = means1, Sigma = covar1), mvrnorm(n = 1, mu = means2, Sigma = covar2), mvrnorm(n = 1, mu = means3, Sigma = covar3))
}
sim_dat <- as.data.frame(cbind(sim_dat,as.factor(k)))

#create a dummy id (change as needed)
sim_dat$id<-paste0("ALSPAC",sprintf('%0.4d', 1:n_ppts))

names(sim_dat)<-c("nara_comp", "nara_acc", "word_acc", "nonword_acc", "wold_comp","k","id")
```

### Missing data

PT: There are different ways to do this and we need to decide which is most realistic for our data. We can remove a percentage of the data completely at random, so random observations are missing, or potentially more likely is that certain individuals will have multiple observations missing, i.e. if there is a missing observation for nara_comp, then I guess its more likely that nara_acc will also be missing too? I've implemented the first option here, but you could add a second step to search for NAs in a row and add extra NAs to that row.

https://stackoverflow.com/questions/50528719/simulate-data-and-randomly-add-missing-values-to-dataframe

**Missing data expectations for this analysis:**
* nara_comp should have no missing data, as inclusion in this/subsequent projects relies on having a comprehension assessment (*unless this should be addressed differently? not quite sure where sample stops otherwise?*)
* nara_acc should have very little missing data - collected at same time as nara_comp, but will be very small proportion due to error
* word_acc & nonword_acc will also have only a very small proportion - collected in same assessment session as nara. Possibly more likely that if word_acc missing then nonword_acc also missing (?), but numbers very small. 
* wold_comp - more likely to have missing data; collected at different clinic session one year earlier 

EJ: For this analysis, it doesn't seem as if there are specific contingencies in missingness (unless we need to incorporate missing nara_comp somehow?), but some variability in likely proportions due to whether the data were collected in same/different clinic visits. I have updated the code to incorporate different proportions.

```{r missing-data}
# specify variables collected in same session, low proportion missingness
same_na_cols <- c("nara_acc", "word_acc", "nonword_acc")
same_na_prop <- 0.01

# specify variables collected in separate session, higher proportion missingness
sep_na_cols <- "wold_comp"
sep_na_prop <- 0.05

# Create dataframe with NAs
sim_dat_NA <- sim_dat %>% 
  pivot_longer(cols = -c(id,k),names_to = "var", values_to = "value") %>%    # pivot data to long format
  mutate(r = runif(nrow(.)),                                            # simulate a random number (r) from 0 to 1 for each row
         value = ifelse(var %in% same_na_cols & r <= same_na_prop, NA,  # for same session vars, update to NA if r < threshold
                        ifelse(var %in% sep_na_cols & r <= sep_na_prop, NA, value))) %>%  # for separate session vars, update to NA is r < threshold, else same value
  dplyr::select(-r) %>%                                                        # remove random number
  pivot_wider(names_from = var, values_from = value)                    # pivot back to original format
```


# Data check

Inspect properties of the variables. 

```{r summarise-vars}
# Quick summary statistics
summary(sim_dat_NA)

#You can get more comprehensive summary statistics using the psych package
describe(sim_dat_NA)


# Check histograms/correlations
pairs.panels(sim_dat_NA)



sim_dat_NA %>% dplyr::select(-id) %>% ggpairs(mapping = aes(color = k))
```

### Missing data

Inspect missingness in the data.

```{r missingness}
vis_miss(sim_dat_NA)
```
(Other tools for visualising relationships in missingness: https://cran.r-project.org/web/packages/naniar/vignettes/getting-started-w-naniar.html)

*...decide what to do with missing data for this analysis.* 

# Fitting multiple models

Fit multiple different models, each with different numbers of latent classes. 

```{r compare-mods}
sim_dat %>% dplyr::select(-c(id,k)) %>%
  estimate_profiles(1:9) %>% 
  compare_solutions(statistics = c("AIC", "BIC", "ICL","SABIC", "Entropy", "BLRT_val", "BLRT_p"))

sim_dat %>% dplyr::select(-c(id,k)) %>%
estimate_profiles(9) %>% plot_profiles() 

```
* *Here I kept increasing the number of profiles until BLRT was no longer significant (?)*  
* *Some disagreement over the best model >> come down to interpretability? Generally favour fewer?* 

tidyLPA outputs a number of different fit indices (described further [here](https://data-edu.github.io/tidyLPA/articles/Introduction_to_tidyLPA.html#getting-fit-statistics)):

* *Definitely want LL, AIC, BIC, bLRT ...select others to prioritise in output?*
* *No Chi-Sqr, CFI, TLI, RMSEA, SRMR - actually not seen reported for latent profile models?* 

### Scree plot

Create plot for visualising fit of different models - just selected AIC and BIC for now, but can incorporate others.

```{r scree-plot}
# Extract fit statistics from multiple models 
n_mods <- sim_dat %>%  dplyr::select(-c(id,k)) %>%                                                 
  estimate_profiles(1:9) %>% 
  get_fit() %>% 
  select(Classes, AIC, BIC) %>% 
  pivot_longer(AIC:BIC, names_to = "statistic", values_to = "value")

# Plot
ggplot(n_mods, aes(x = factor(Classes), y = value)) + 
  geom_point() +
  geom_line(group = 1) + 
  xlab("n classes") + 
  theme_bw() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.y = element_blank()) +
  facet_grid(statistic ~., scales = "free")
```

### Model specification 

Additional options for whether/how variances and covariances are estimated. Six different models can be fitted (if have Mplus, only four options without). Information [here](https://data-edu.github.io/tidyLPA/articles/Introduction_to_tidyLPA.html#model-specification). 

*Not sure I understand this too well yet, and whether it's a case of trying all the options for the best fit or theoretically guided?
Can produce very different results... for example, for fitting all combinations for up to 6 classes... *

```{r compare-mod-specs}
sim_dat %>%
  estimate_profiles(1:6,
                    variances = c("equal", "varying", "equal", "varying"),
                    covariances = c("zero", "zero", "equal", "varying")) %>%
  compare_solutions(statistics = c("AIC", "BIC", "BLRT_val", "BLRT_p"))
```

# Model profiles

Once have final model (or exploring individual solution for interpetability), explore resultant profiles. 

First fit selected model separately - e.g., for 8-class model (with covariances constrained to 0).

```{r fit-selected}
selected_mod <- sim_dat %>% 
  estimate_profiles(8)
```

Plot profiles - package option quite hard to interpret, so also plotted own version...

```{r plot-profiles}
# tidyLPA option
plot_profiles(selected_mod, rawdata = FALSE)

# Extract estimates, reorder tasks for interpretability 
estimates <- get_estimates(selected_mod) %>% 
  filter(Category == "Means") %>% 
  mutate(Class = factor(Class),
         Parameter = factor(Parameter, levels = c("nonword_acc", "word_acc", "nara_acc", "nara_comp", "wold_comp")))

# Plot mean test scores for each class
ggplot(estimates, aes(x = Parameter, y = Estimate, colour = Class)) + 
  geom_point() + 
  geom_line(aes(group = Class)) +
  xlab("Measure") +
  #geom_errorbar(aes(ymin = Estimate - se, ymax = Estimate + se), width = .1) +
  theme_bw()
```

A poor comprehender profile would have higher scores on the left 3 measures, low scores on the last 2 (e.g., here classes 4, 8?). 

Should think about extent to which want to differentiate levels of performance - e.g., a 4-class model might produce classes aligned with the Simple View, but probably do want to focus on children with *weaknesses* in comprehension? 

# Extracting profile information

Each participant is given a probability of membership to each class, based on their task scores. 

```{r class-assign}
class_dat <- get_data(selected_mod)
```

Inspect classification utility - look at how well classes are distinguished from each other

```{r classification-table}
class_dat %>% 
  group_by(Class) %>% 
  summarise(across(CPROB1:CPROB8, mean)) %>% 
  round(2)
```

Summarise class assignments, and check that sample statistics per class conform to parameters estimated by model. 

```{r class-summary}
# Probability of being assigned to each class
colMeans(class_dat[8:15])

# Number and proportion of individuals assigned to each class
class_dat %>% 
  group_by(Class) %>% 
  summarise(group_n = n()) %>% 
  mutate(percent_sample = (group_n / sum(group_n))*100)

# Class descriptives
class_dat %>% 
  select(Class, nonword_acc, word_acc, nara_acc, nara_comp, wold_comp) %>% 
  group_by(Class) %>% 
  summarise(across(nonword_acc:wold_comp, list(mean = mean, sd = sd)))
```

*Consider threshold for class assignment?* - e.g., Foorman et al (2017) paper only included participants who had >.70 probability for their final class. Can explore range of final assignment...?

```{r class-prob}
# Create column with probability of selected class
class_dat <- class_dat %>%
  mutate(class_prob = ifelse(Class == 1, CPROB1,
                             ifelse(Class == 2, CPROB2,
                                    ifelse(Class == 3, CPROB3,
                                           ifelse(Class == 4, CPROB4,
                                                  ifelse(Class == 5, CPROB5,
                                                         ifelse(Class == 6, CPROB6,
                                                                ifelse(Class == 7, CPROB7,
                                                                       CPROB8))))))))
# Summarise probabilities
class_dat %>%
  group_by(Class) %>%
  summarise(min_prob = min(class_prob), max = max(class_prob), mean = mean(class_prob), median = median(class_prob))

# Visualise probabilities??? 
class_dat %>%
  ggplot(aes(class_prob, group = Class)) +
  geom_density(aes(colour = factor(Class))) +
  theme_bw()
```

